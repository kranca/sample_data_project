{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff3159b-b57f-49dd-98e4-b41fb10f9245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "class DataProductConfig:\n",
    "    \"\"\"\n",
    "    Configuration class to manage allowed schemas and enforce schema validation\n",
    "    for writing Spark tables in Databricks.\n",
    "    \"\"\"\n",
    "\n",
    "    ALLOWED_CATALOGS = {\n",
    "        \"bronze\": \"1_data_sources\",\n",
    "        \"silver\": \"2_data_products\",\n",
    "        \"gold\": \"3_analytics_products\",\n",
    "    }\n",
    "\n",
    "    PREFIXES = {\n",
    "        \"1_data_sources\": \"ds\",\n",
    "        \"2_data_products\": \"dp\",\n",
    "        \"3_analytics_products\": \"ap\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, table_name: str, schema: str, layer: str):\n",
    "        \"\"\"\n",
    "        Initialize DataProductConfig.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Original table name (may contain spaces or special characters).\n",
    "            schema (str): Schema name where the table will be stored.\n",
    "            layer (str): Data layer name (bronze, silver, gold).\n",
    "        \"\"\"\n",
    "        if layer not in self.ALLOWED_CATALOGS:\n",
    "            raise ValueError(f\"Invalid layer '{layer}'. Must be one of: {list(self.ALLOWED_CATALOGS)}\")\n",
    "\n",
    "        self.table_name = table_name\n",
    "        self.schema = schema\n",
    "        self.catalog = self.ALLOWED_CATALOGS[layer]\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_table_name(table_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean a table name string by:\n",
    "        - Removing special characters\n",
    "        - Replacing spaces and dots with underscores\n",
    "        - Lowercasing all letters\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Raw table name.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned table name.\n",
    "        \"\"\"\n",
    "        p = Path(table_name)\n",
    "        # convert to lower caseand replace non alphanumerical characters\n",
    "        clean_name = re.sub(r\"[^a-z0-9]+\", \"_\", p.stem.lower()).strip(\"_\")\n",
    "        return clean_name\n",
    "\n",
    "    def _table_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Cleans and standardizes the table name according to naming conventions.\n",
    "\n",
    "        Returns:\n",
    "            str: Final standardized table name with the correct prefix.\n",
    "        \"\"\"\n",
    "        prefix = self.PREFIXES[self.catalog]\n",
    "        if self.table_name.startswith(f\"{prefix}_\"):\n",
    "            return self._clean_table_name(self.table_name)\n",
    "        return self._clean_table_name(f\"{prefix}_{self.table_name}\")\n",
    "\n",
    "    def write_table(self, df: DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Writes a Spark DataFrame to a table in the specified catalog and schema.\n",
    "        The table name is cleaned and prefixed according to conventions.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Spark DataFrame to write.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Create new schema if it doesn't already exist\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.catalog}.{self.schema}\")\n",
    "\n",
    "        # clean table name\n",
    "        final_name = self._table_name()\n",
    "        full_path = f\"{self.catalog}.{self.schema}.{final_name}\"\n",
    "        print(f\"Writing DataFrame to table: {full_path}\")\n",
    "        df.write.mode(\"overwrite\").saveAsTable(full_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "table_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
