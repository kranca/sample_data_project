{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c0c27f-55f4-4eff-9c5a-41964561ad4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def clean_column_names(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes column names in a DataFrame for easier processing and consistency.\n",
    "\n",
    "    Transformations applied:\n",
    "    - Lowercases all letters\n",
    "    - Replaces spaces and any non-alphanumeric characters with underscores\n",
    "    - Collapses multiple underscores into one\n",
    "    - Removes leading and trailing underscores\n",
    "\n",
    "    Args:\n",
    "    df (pyspark.sql.DataFrame): DataFrame to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    pyspark.sql.DataFrame: A new DataFrame with cleaned and standardized column names.\n",
    "    \"\"\"\n",
    "    cleaned_cols = []\n",
    "    for col in df.columns:\n",
    "        col_clean = col.lower()\n",
    "        # replace any sequence of non-alphanumeric chars with underscore\n",
    "        col_clean = re.sub(r'[^a-z0-9]+', '_', col_clean)\n",
    "        # remove leading/trailing underscores\n",
    "        col_clean = col_clean.strip('_')\n",
    "        cleaned_cols.append(col_clean)\n",
    "    \n",
    "    # rename columns in the DataFrame\n",
    "    for old, new in zip(df.columns, cleaned_cols):\n",
    "        df = df.withColumnRenamed(old, new)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# def clean_decimals(df: DataFrame, columns:, null_values: Optional[str] = None) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Cleans columns of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ddfa8c7-bf02-4795-8173-f79f4fb5b8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, regexp_replace, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def clean_decimal_columns(\n",
    "    df: DataFrame,\n",
    "    columns: Union[str, List[str]],\n",
    "    replace_nulls: Optional[Union[str, List[str]]] = None,\n",
    "    decimal_separator: str = \".\",\n",
    "    thousands_separator: str = \",\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans one or more decimal columns stored as strings:\n",
    "      - Replaces specified string values with null\n",
    "      - Removes thousands separators\n",
    "      - Converts decimal separator to dot\n",
    "      - Casts column to DoubleType\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Spark DataFrame to transform.\n",
    "        columns (str | list[str]): Column name or list of columns to clean.\n",
    "        replace_nulls (list[str], optional): List of string values to replace with null before casting.\n",
    "        decimal_separator (str, optional): Decimal separator in the string values (default \".\").\n",
    "        thousands_separator (str, optional): Thousands separator in the string values (default \",\").\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Transformed Spark DataFrame with cleaned decimal columns.\n",
    "    \"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    for col_name in columns:\n",
    "        cleaned_col = col(col_name)\n",
    "\n",
    "        # Replace specified values with null\n",
    "        if replace_nulls:\n",
    "            if isinstance(replace_nulls, str):\n",
    "                replace_nulls = [replace_nulls]\n",
    "            for val in replace_nulls:\n",
    "                df = df.withColumn(col_name, when(cleaned_col == val, None).otherwise(cleaned_col))\n",
    "\n",
    "        # Remove thousands separator and normalize decimal separator\n",
    "        if thousands_separator:\n",
    "            # df = df.withColumn(col_name, regexp_replace(cleaned_col, f\"\\\\{thousands_separator}\", \"\"))\n",
    "            cleaned_col = regexp_replace(cleaned_col, f\"\\\\{thousands_separator}\", \"\")\n",
    "\n",
    "        if decimal_separator != \".\":\n",
    "            # df = df.withColumn(col_name, regexp_replace(cleaned_col, f\"\\\\{decimal_separator}\", \".\"))\n",
    "            cleaned_col = regexp_replace(cleaned_col, f\"\\\\{decimal_separator}\", \".\")\n",
    "\n",
    "        # Cast to double\n",
    "        # df = df.withColumn(col_name, cleaned_col.cast(\"double\"))\n",
    "        cleaned_col = cleaned_col.cast(DoubleType())\n",
    "\n",
    "        df = df.withColumn(col_name, cleaned_col)\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataframe_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
